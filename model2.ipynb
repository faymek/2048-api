{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = []\n",
    "with open(\"trainset2.csv\") as f:\n",
    "    for i in range(70000):\n",
    "        data_raw.append(eval(f.readline()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.array(data_raw)\n",
    "images = dataset[:,:-1].astype(\"float32\")\n",
    "labels = dataset[:,-1].astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 10000\n",
    "batch_size = 128\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 64 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 4 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.25 # Dropout, probability to drop a unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the neural network\n",
    "def conv_net(x_dict, n_classes, dropout, reuse, is_training):\n",
    "    \n",
    "    # Define a scope for reusing the variables\n",
    "    with tf.variable_scope('ConvNet', reuse=reuse):\n",
    "        # TF Estimator input is a dict, in case of multiple inputs\n",
    "        x = x_dict['images']\n",
    "\n",
    "        # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
    "        # Reshape to match picture format [Height x Width x Channel]\n",
    "        # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "        x = tf.reshape(x, shape=[-1, 8, 8, 1])\n",
    "\n",
    "        # Convolution Layer with 32 filters and a kernel size of 3\n",
    "        conv1 = tf.layers.conv2d(x, 32, 4, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        #conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n",
    "\n",
    "        # Convolution Layer with 64 filters and a kernel size of 3\n",
    "        conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        #conv2 = tf.layers.max_pooling2d(conv2, 2, 2)\n",
    "\n",
    "        # Flatten the data to a 1-D vector for the fully connected layer\n",
    "        fc1 = tf.contrib.layers.flatten(conv2)\n",
    "\n",
    "        # Fully connected layer (in tf contrib folder for now)\n",
    "        fc1 = tf.layers.dense(fc1, 1024)\n",
    "        # Apply Dropout (if is_training is False, dropout is not applied)\n",
    "        fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)\n",
    "\n",
    "        # Output layer, class prediction\n",
    "        out = tf.layers.dense(fc1, n_classes)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the model function (following TF Estimator Template)\n",
    "def model_fn(features, labels, mode):\n",
    "    \n",
    "    # Build the neural network\n",
    "    # Because Dropout have different behavior at training and prediction time, we\n",
    "    # need to create 2 distinct computation graphs that still share the same weights.\n",
    "    logits_train = conv_net(features, num_classes, dropout, reuse=False, is_training=True)\n",
    "    logits_test = conv_net(features, num_classes, dropout, reuse=True, is_training=False)\n",
    "    \n",
    "    # Predictions\n",
    "    pred_classes = tf.argmax(logits_test, axis=1)\n",
    "    pred_probas = tf.nn.softmax(logits_test)\n",
    "    \n",
    "    # If prediction mode, early return\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=pred_classes) \n",
    "        \n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits_train, labels=tf.cast(labels, dtype=tf.int32)))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op, global_step=tf.train.get_global_step())\n",
    "    \n",
    "    # Evaluate the accuracy of the model\n",
    "    acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "    \n",
    "    # TF Estimators requires to return a EstimatorSpec, that specify\n",
    "    # the different ops for training, evaluating, ...\n",
    "    estim_specs = tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      predictions=pred_classes,\n",
    "      loss=loss_op,\n",
    "      train_op=train_op,\n",
    "      eval_metric_ops={'accuracy': acc_op})\n",
    "\n",
    "    return estim_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './mymodels/model3', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "# Build the Estimator\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "model = tf.estimator.Estimator(model_fn=model_fn,model_dir=\"./mymodels/model3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from ./mymodels/model3/model.ckpt-30000\n",
      "INFO:tensorflow:Saving checkpoints for 30001 into ./mymodels/model3/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.9111126, step = 30001\n",
      "INFO:tensorflow:global_step/sec: 46.1969\n",
      "INFO:tensorflow:loss = 0.81463295, step = 30101 (2.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 45.9717\n",
      "INFO:tensorflow:loss = 0.9182801, step = 30201 (2.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.2986\n",
      "INFO:tensorflow:loss = 0.8761561, step = 30301 (2.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.0249\n",
      "INFO:tensorflow:loss = 0.8133836, step = 30401 (2.173 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.1703\n",
      "INFO:tensorflow:loss = 0.8868187, step = 30501 (2.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.1563\n",
      "INFO:tensorflow:loss = 0.98359877, step = 30601 (2.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.4054\n",
      "INFO:tensorflow:loss = 0.79388845, step = 30701 (2.155 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.0687\n",
      "INFO:tensorflow:loss = 0.8046915, step = 30801 (2.322 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.1772\n",
      "INFO:tensorflow:loss = 0.79764897, step = 30901 (2.316 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.7293\n",
      "INFO:tensorflow:loss = 0.871319, step = 31001 (2.140 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.0261\n",
      "INFO:tensorflow:loss = 0.8141749, step = 31101 (2.173 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.4545\n",
      "INFO:tensorflow:loss = 0.8330983, step = 31201 (2.153 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.3049\n",
      "INFO:tensorflow:loss = 0.79652363, step = 31301 (2.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 45.4336\n",
      "INFO:tensorflow:loss = 0.824574, step = 31401 (2.201 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.3488\n",
      "INFO:tensorflow:loss = 0.7063797, step = 31501 (2.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 45.8654\n",
      "INFO:tensorflow:loss = 0.8499219, step = 31601 (2.180 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.2667\n",
      "INFO:tensorflow:loss = 0.8108213, step = 31701 (2.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.3201\n",
      "INFO:tensorflow:loss = 0.9639065, step = 31801 (2.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.0678\n",
      "INFO:tensorflow:loss = 0.80019784, step = 31901 (2.171 sec)\n",
      "INFO:tensorflow:global_step/sec: 45.4628\n",
      "INFO:tensorflow:loss = 0.8295417, step = 32001 (2.200 sec)\n",
      "INFO:tensorflow:global_step/sec: 44.7583\n",
      "INFO:tensorflow:loss = 0.7352934, step = 32101 (2.235 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.7542\n",
      "INFO:tensorflow:loss = 0.8297778, step = 32201 (2.285 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.0224\n",
      "INFO:tensorflow:loss = 0.8774389, step = 32301 (2.173 sec)\n",
      "INFO:tensorflow:global_step/sec: 45.531\n",
      "INFO:tensorflow:loss = 0.85483575, step = 32401 (2.196 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.3266\n",
      "INFO:tensorflow:loss = 0.865905, step = 32501 (2.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.1929\n",
      "INFO:tensorflow:loss = 0.8308267, step = 32601 (2.165 sec)\n",
      "INFO:tensorflow:global_step/sec: 45.8323\n",
      "INFO:tensorflow:loss = 0.74791557, step = 32701 (2.182 sec)\n",
      "INFO:tensorflow:global_step/sec: 45.9167\n",
      "INFO:tensorflow:loss = 0.92479515, step = 32801 (2.178 sec)\n",
      "INFO:tensorflow:global_step/sec: 45.9518\n",
      "INFO:tensorflow:loss = 0.7914788, step = 32901 (2.177 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.5876\n",
      "INFO:tensorflow:loss = 0.83391607, step = 33001 (2.146 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.7245\n",
      "INFO:tensorflow:loss = 0.89332753, step = 33101 (2.140 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.1574\n",
      "INFO:tensorflow:loss = 0.8227185, step = 33201 (2.167 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.1413\n",
      "INFO:tensorflow:loss = 0.8175781, step = 33301 (2.167 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.412\n",
      "INFO:tensorflow:loss = 0.96902597, step = 33401 (2.155 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.6979\n",
      "INFO:tensorflow:loss = 0.7714565, step = 33501 (2.141 sec)\n",
      "INFO:tensorflow:global_step/sec: 45.6538\n",
      "INFO:tensorflow:loss = 0.83014995, step = 33601 (2.190 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.2873\n",
      "INFO:tensorflow:loss = 0.9395975, step = 33701 (2.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.5609\n",
      "INFO:tensorflow:loss = 0.84747404, step = 33801 (2.148 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.3728\n",
      "INFO:tensorflow:loss = 0.9062867, step = 33901 (2.157 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 34000 into ./mymodels/model3/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.7417532.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x7f8f847a2208>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the input function for training\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': images}, y=labels,\n",
    "    batch_size=batch_size, num_epochs=None, shuffle=True)\n",
    "# Train the Model\n",
    "model.train(input_fn, steps=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-12-12-01:16:42\n",
      "INFO:tensorflow:Restoring parameters from ./mymodels/model3/model.ckpt-34000\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-12-01:16:47\n",
      "INFO:tensorflow:Saving dict for global step 34000: accuracy = 0.59594285, global_step = 34000, loss = 0.9065399\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.59594285, 'global_step': 34000, 'loss': 0.9065399}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the Model\n",
    "# Define the input function for evaluating\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': images}, y=labels,\n",
    "    batch_size=batch_size, shuffle=False)\n",
    "# Use the Estimator 'evaluate' method\n",
    "model.evaluate(input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./mymodels/model.ckpt-10287129\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8f84432ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMQOXGjNwqTEDlxozcKmx/wUeDN1iweV/FwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8f84a1f438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLjVm4FJjBi41ZuBSYwYuNfYH2KrdYkGowAIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8f848d4908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8f848109e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction: 3\n"
     ]
    }
   ],
   "source": [
    "# Predict single images\n",
    "n_images = 4\n",
    "# Get images from test set\n",
    "test_images = images[1000:1000+n_images]\n",
    "# Prepare the input data\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': images}, shuffle=False)\n",
    "# Use the model to predict the images class\n",
    "preds = list(model.predict(input_fn))\n",
    "\n",
    "# Display\n",
    "for i in range(n_images):\n",
    "    plt.imshow(np.reshape(test_images[i], [8, 8]), cmap='gray')\n",
    "    plt.show()\n",
    "    print(\"Model prediction:\", preds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
